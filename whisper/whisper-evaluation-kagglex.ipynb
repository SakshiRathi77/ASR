{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluation Notebook\n\n## Objective\nThe main objective of this notebook is to evaluate the accuracy and performance of the Whisper model trained for speech recognition.\n","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install jiwer accelerate datasets huggingface_hub transformers","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import load_dataset, load_metric,Dataset\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nimport re\nimport unicodedata\nimport pandas as pd\nimport soundfile as sf","metadata":{"execution":{"iopub.status.busy":"2023-10-17T16:43:03.844741Z","iopub.execute_input":"2023-10-17T16:43:03.845890Z","iopub.status.idle":"2023-10-17T16:43:26.092273Z","shell.execute_reply.started":"2023-10-17T16:43:03.845845Z","shell.execute_reply":"2023-10-17T16:43:26.091084Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\nThe evaluation is conducted on the dataset used during the training process, which can be found [here](https://huggingface.co/datasets/SakshiRathi77/ASR_CV15_Hindi_wav_16000).","metadata":{}},{"cell_type":"code","source":"df =pd.read_csv(\"/kaggle/input/cv15-hindi/hi/hi/train.tsv\", sep='\\t', header=0)\ndf[\"votes\"] = df[\"up_votes\"]-df[\"down_votes\"]\ndf = df[df[\"votes\"]>=2]\ndf[\"path\"]=df[\"path\"].str.replace(\".mp3\",\".wav\")","metadata":{"execution":{"iopub.status.busy":"2023-10-17T16:43:26.096850Z","iopub.execute_input":"2023-10-17T16:43:26.097117Z","iopub.status.idle":"2023-10-17T16:43:26.195808Z","shell.execute_reply.started":"2023-10-17T16:43:26.097095Z","shell.execute_reply":"2023-10-17T16:43:26.194844Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Model Information\nThe evaluation utilizes the Wav2Vec2-XLSR model, which has been trained on the provided dataset. The details of the training process can be found in the [training notebook](https://www.kaggle.com/code/sakshirathi77/wav2vec2-xlsr-kagglex).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf[\"path\"] = \"/kaggle/input/cv15-hindi/audio_wav_16000/tmp/CV15_ASR_dataset/audio_wav_16000/\"+df[\"path\"]\ndf.rename(columns = {'transcription':'sentence'}, inplace = True)\ntrain,test = train_test_split(df, test_size=0.1, random_state=42)\ncommon_voice_test = Dataset.from_pandas(test)\n\nwer = load_metric(\"wer\")\ncer = load_metric(\"cer\")\n\nprocessor = WhisperProcessor.from_pretrained(\"kingabzpro/whisper-small-hi-cv\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"kingabzpro/whisper-small-hi-cv\").to(\"cuda\")\n\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = sf.read(batch[\"path\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ncommon_voice_test = common_voice_test.map(speech_file_to_array_fn)\n\ndef map_to_pred(batch):\n\n    input_features = processor(batch[\"speech\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n    batch[\"reference\"] = processor.tokenizer._normalize(batch['sentence'])\n\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n    transcription = processor.decode(predicted_ids)\n    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n    return batch\n\nresult = common_voice_test.map(map_to_pred)\n\nprint(\"WER: {:2f}\".format(wer.compute(predictions=result[\"prediction\"], references=result[\"reference\"])))\nprint(\"CER: {:2f}\".format(cer.compute(predictions=result[\"prediction\"], references=result[\"reference\"])))","metadata":{"execution":{"iopub.status.busy":"2023-10-17T16:44:15.035492Z","iopub.execute_input":"2023-10-17T16:44:15.035905Z","iopub.status.idle":"2023-10-17T16:44:21.287039Z","shell.execute_reply.started":"2023-10-17T16:44:15.035869Z","shell.execute_reply":"2023-10-17T16:44:21.285929Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/416 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076b37b8582243ac81bfbc8039bce497"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation Metrics\nThe following metrics are used for evaluating the performance of the model:\n- Word Error Rate (WER) -0.139913 \n- Character Error Rate (CER)-  0.058844\n\n\nThe assessment of the Whisper model in the ASR task has delivered compelling outcomes, demonstrating a Word Error Rate (WER) of 0.139913 and Character Error Rate (CER) of 0.058844. These results underscore the model's remarkable precision and resilience in converting speech to text, showcasing its effectiveness in managing intricate linguistic subtleties and a wide range of speech patterns within the evaluated dataset.\n\nThe WER and CER values highlight the model's elevated accuracy and reliability, suggesting its potential utility in diverse real-world settings that demand precise and swift speech-to-text transcription. Furthermore, the evaluation has underscored the model's adeptness in minimizing errors and upholding the fidelity of the original speech input.\n\nAlthough the current findings are impressive, there are opportunities for further advancement to enhance the model's performance. These may include refining fine-tuning techniques, exploring additional data augmentation methods, and integrating sophisticated language modeling approaches. These efforts could potentially contribute to even greater accuracy and robustness, cementing the Whisper model's status as a cutting-edge solution in the domain of Automatic Speech Recognition.","metadata":{}}]}